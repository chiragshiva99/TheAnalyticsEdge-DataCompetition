---
title: "TAE Data Competition"
author: "Chirag Shivakumar and Tan Zen Sheen"
date: "2023-09-02"
output: html_document
---

# The Analytics Edge Data Competition, Twitter Sentiment Analysis

In this project, we are given a labelled dataset of Tweets from twitter.
In the train.csv data, tweets have been labelled with a Sentiment score of -1, 0, or 1, representing sentiments of negative, neutral or positive respectively.

Through applying various Machine Learning Techniques, my team and I are tasked to create a data model to use the Data Model to make predictions on train.csv dataset.

# Load Packages needed for this Project, install if needed.
```{r}
library(stringr) 
library(spellcheckr)
library(dplyr)
library(tm)
library(SnowballC)
library(ROCR)
library(caTools)
library(glmnet)
library(nnet)
library(randomForest)
library(keras)
library(deepviz)
```

# Data Cleaning and Processing
In cleaning and processing the data of tweets, we decided to go one step further, extracting information of emoticons from the tweets, removing website links, and detect some incorrectly spelled words.
The entire script for Data Cleaning and Processing is saved in clean.R.

```{r cars}
source("clean.R")
```

# How clean.R it works

## Extracting of punctuations and emoticons from Tweets and assigning value to new columns that represent certain emotions.
Besides organizing the streets of tweets into a dataframe that stores the frequency of words in each tweet, clean.R helps to extract the emoticons in the tweets by detecting them and assigning a value to its associated emotion.

To do this, we add new columns of sadd, smle, qsnm, excl and dott to the output dataframe. The new columns are computed to have the following meanings:
*- sadd: Means 'sad' and refers to the emotion of sadness. This column will have a value of 1 when there is an emoji that represents the emotion of sadness, anger or disappointment. *
*- smle: Means 'smile' and refers to the emotion of happiness. This column will have a value of 1 when there is an emoji that represents the emotion of happiness or excitement. *
*- excl: Refers to the exclamation mark "!". This column will have a value of 1 when there is a "!" in the tweet. *
*- qsm : Refers to question mark "?". This column will have a value of 1 when there is a "?" in the tweet.*
*- dott: Refers to multiple continuous dots in tweets. This column will have a value of 1 when there are consecutive periods in a tweet(I.e. "...")*

Using the emoticons data set from https://github.com/matbun/Emoticons-dataset , tweets from the data are being screened through to detect emoticons. The emoticons dataset has actually been labelled with values of 1 or -1, representing positive and negative sentiments respectively.
Using these labels, we associate the positive emotions (that are labelled '1') to have a smle value of 1 and a negative emoticons (that are labelled '-1') to have the sadd value of 1. If there are no emoticons present in the tweet, the 'smle' and 'sadd' columns would be labelled 0.

```{r}
emo_data <- read.csv("emoticons_long.csv")
print( head(emo_data))
```

## Identifying punctuation through clean_punc function from clean.R
In clean.R, the *clean_punc* function takes in a string and outputs a list which contains *"$words'* and *'$punc'*.
The *'$punc'* output variable stores the computed columns of *'sadd'*, *'smle'*, *'qsnm'*, *'excl'*, and *'dott'*.

```{r}
# test code for clean_punc function

str1 <- clean_punc('I love Machine Learning!:)')
print( str1$punc)

str2 <- clean_punc('Why is my model takign so long to train...? :(')
print( str2$punc)

```

## Correction of words with 3 of the same consecutive characters and removal of website url in Tweets.
As the tweets contain numerous typos, it is wise to do some cleaning of the tweets to ensure that the words extracted are correcly spelled as much as possible.
As running a spelling checker through all the words in the tweet would be computationally very intensive, we decided to simplify this by simply checking for words that have 3 of the same consecutive letters.
Website URLs are also removed from the tweets as they are not important for the prediction of the model
```{r}

str3 <- clean_punc( " yeeeeah .. . and it was in into too  seen Empire top 100 computer games? http://www.empireonline.com/100greatestgames/" )

print( str3$words)

str4 <- clean_punc("i kno look ? :) (: i doooo!!!!!!!!!! yall partyin with out me htttp://www.google.com" )
print( str4$words)

```

## Execute Cleaning and Data Processing on test and train datasets.
Note of Caution: Running the code below will take 20-30 minutes
In the original competition, an unlabelled test data set, test_data.csv is given for us to make our predictions on, which would be submitted on to Kaggle. As the labels from the test data are not available, we shalll ignore the test data and instead split the labelled data for training and testing. 
The model will be trained on the splitted training data and tested on the splitted testing data.
```{r}

train <- prep_data( "train.csv" )
train_data <- train$data
train_data$sentiment <- train$sentiment
write.csv( train_data,"train_data.csv", row.names = FALSE)


```

# Load Processed Data and split data for Training and Testing
```{r}
data <- read.csv("train_data.csv")


# Function to get unique values of a column
get_unique_values <- function(column) {
  unique_values <- unique(column)
  return(unique_values)
}

# Initialize an empty list to store unique values for each column
unique_values_list <- list()

# Flatten the dataframe into a vector
data_vector <- unlist(subset(data , select = -c(sentiment) ))

# Get unique values from the vector
unique_values <- unique(data_vector)

print( unique_values )

has_null_values <- anyNA(data)
# Print the result (TRUE if any missing values, FALSE otherwise)
print(has_null_values)


# twitter$tweet[25]
data[25,]
set.seed(123)
split_indices  <- sample.split(data , SplitRatio=0.7)

train_data <- data[split_indices, ]
test_data <- data[!split_indices, ]

```


# First Model
In the first model, we will predicting neutral and non-neutral (positive or negative) tweets, before predicting positive and negative tweets.

The motivation behind this model is that based on the principle that tweets that are less likely to negative are more likely to be positive, and vice versa.

The methodology of this model is to first create a subset of the 


## Data Preparation for 1st Model
```{r}
# make copies of train and data so as to modify orignial data
train_lr <- train_data
test_lr <- test_data

# Assign Positive, Neutral and Negative columns to training data, based on Sentiment Scores
train_lr$Neutral <-as.factor(train_data$sentiment==0)
train_lr$NonNeutral <-as.factor(train_data$sentiment!=0)

# Do the same for test data, for testing of model
test_lr$Neutral <-as.factor(test_data$sentiment==0)
test_lr$NonNeutral <-as.factor(test_data$sentiment!=0)

# Data for Neutral Model
train_Neutral <- subset( train_lr , select = -c(sentiment,NonNeutral) )
test_Neutral <- subset( test_lr , select = -c(sentiment,NonNeutral) )

# Data for Non-Neutral Model
# remove neutral tweet, so we are now left with positive and negative tweets
train_Non_Neutral <- train_lr[ train_lr$NonNeutral == TRUE , ]
test_Non_Neutral <- test_lr[ test_lr$NonNeutral == TRUE , ]

train_Non_Neutral$Positive <- as.factor( train_Non_Neutral$sentiment == 1 )
test_Non_Neutral$Positive <- as.factor( test_Non_Neutral$sentiment == 1 )

train_Non_Neutral <- subset( train_Non_Neutral , select = -c(Neutral , NonNeutral , sentiment ) )
test_Non_Neutral <- subset( test_Non_Neutral , select = -c(Neutral , NonNeutral , sentiment ) )

```

## Neutral Model, seperating neutral from non-neutral tweet
```{r}
# Logistic regression
# ---------------------------Neutral Model--------------------------- 
modelNeutral <- glm(Neutral~0+., data=train_Neutral, family=binomial)
# summary(modelNeutral) # AIC: 17588

neutral_pVal <- summary(modelNeutral)$coefficients[,4] #p values
neutral_sigVal <- names(neutral_pVal[neutral_pVal <= 0.01]) #1% level

modelNeutral_2 <- glm(as.formula(
  paste0("Neutral ~ ",
         paste0(neutral_sigVal, collapse = " + "),
         "-1")  
) , data = train_Neutral, family = binomial)
# summary(modelNeutral_2) #AIC: 17435
```

## Non-Neutral Model, Seperating Positive tweets from Negative Tweets
```{r}

modelPositive <- glm(Positive~0+., data=train_Non_Neutral, family=binomial)


positive_pVal <- summary(modelPositive)$coefficients[,4] #p values
positive_sigVal <- names(positive_pVal[positive_pVal <= 0.01]) #1% level

modelPositive_2 <- glm(as.formula(
  paste0("Positive ~ ",
         paste0(positive_sigVal, collapse = " + "),
         "-1")  
) , data = train_Non_Neutral, family = binomial)

# summary(modelPositive_2) #AIC: 6403.1
```


## Model Predictions for 1st Model 

**NEED HELP TO EDIT THIS PART. MODEL IS MEANT TO RUN ON THE TRAINING DATA WITHOUT SPLITTING THE NEUTRAL AND NON-NEUTRAL LABELS.
RATHER IT SHOULD HAVE THE PREDICTED NEUTRAL AND NON-NEUTRAL DATA SPLIT FROM THE FIRST MODEL, THEN POSITIVE AND NEGATIVE DATA SPLIT FROM THE SECOND MODEL.
```{r}
# # Non-Neutral Model
# predict_pn <- predict( modelPositive_2, test_Non_Neutral , type = 'response' ) 
# 
# tab_pn <- table(predict_pn>0.53,test_Non_Neutral$Positive) #CM
# Accuracy = (tab_pn[1,1]+tab_pn[2,2])/sum(tab_pn)
# Accuracy # [1] 0.8128718
# 
# library(ROCR)
# 
# print( length(predict_pn ))
# print( length(test_Non_Neutral$Positive) )
# 
# predict_pn1 <- prediction( predict_pn , test_Non_Neutral$Positive )
# perf_pn <- performance(predict_pn1,x.measure="fpr",measure="tpr")
# plot(perf_pn,colorize=T,print.cutoffs.at=c(0,0.1,0.2,0.3,0.5,0.6,0.7,0.8,0.9,1),text.adj=c(-.02,1.7))
# 
# 
# # Neutral Model
# predict_nn <- predict( modelNeutral_2 , test_Neutral , type = 'response' ) 
# tab_nn <- table(predict_nn>0.49,test_Neutral$Neutral) #CM
# Accuracy = (tab_nn[1,1]+tab_nn[2,2])/sum(tab_nn)
# Accuracy # [1] 0.69073
# 
# print( length(predict_nn) )
# print( length(test_Neutral$Neutral) )
# 
# predict_nn1 <- prediction( predict_nn , test_Neutral$Neutral )
# perf_nn <- performance(predict_pn1,x.measure="fpr",measure="tpr")
# plot(perf_nn,colorize=T,print.cutoffs.at=c(0,0.1,0.2,0.3,0.5,0.6,0.7,0.8,0.9,1),text.adj=c(-.02,1.7))
```

### TEST CODE

```{r}
# Predict Neutral vs. Non-Neutral Labels
predict_nn <- predict(modelNeutral_2, test_Neutral, type = 'response')

# Calculate Accuracy for Neutral vs. Non-Neutral
tab_nn <- table(predict_nn>0.49, test_Neutral$Neutral)
accuracy_nn <- sum(diag(tab_nn)) / sum(tab_nn)
accuracy_nn

# Confusion Matrix for Neutral vs. Non-Neutral
tab_nn

# Add predicted neutral and non-neutral labels to the test_Neutral dataframe
test_Neutral$Neutral <- as.factor(predict_nn>0.49)
test_Neutral$NonNeutral <- as.factor(!predict_nn>0.49)

# ROC Curve for Neutral vs. Non-Neutral
predict_nn1 <- prediction(predict_nn, test_Neutral$Neutral)
perf_nn <- performance(predict_nn1, x.measure = "fpr", measure = "tpr")
plot(perf_nn, colorize = TRUE, print.cutoffs.at = c(0, 0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1), text.adj = c(-.02, 1.7))
```

```{r}
# Predict Non-Neutral (Positive vs. Negative) Labels
predict_pn <- predict(modelPositive_2, test_Non_Neutral, type = 'response')

# Calculate Accuracy for Positive vs. Negative
tab_pn <- table(predict_pn>0.53, test_Non_Neutral$Positive)
accuracy_pn <- sum(diag(tab_pn)) / sum(tab_pn)
accuracy_pn

# Confusion Matrix for Positive vs. Negative
tab_pn

# Add predicted positive and negative labels to the test_Non_Neutral dataframe
test_Non_Neutral$Positive <- as.factor(predict_pn>0.53)
test_Non_Neutral$Negative <- as.factor(!predict_pn>0.53)


# ROC Curve for Positive vs. Negative
predict_pn1 <- prediction(predict_pn, test_Non_Neutral$Positive)
perf_pn <- performance(predict_pn1, x.measure = "fpr", measure = "tpr")
plot(perf_pn, colorize = TRUE, print.cutoffs.at = c(0, 0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1), text.adj = c(-.02, 1.7))
```




```{r}
# First perform binary classification to predict whether a tweet is neutral or non-neutral
# Then for the non-neutral tweets, you can further classify them as positive or negative.

# Step 1: Binary Classification for Neutral vs. Non-Neutral
# Use this modelNeutral_2 model to predict probabilities of being neutral for all tweets in your test dataset (test_data).
predict_neutral <- predict(modelNeutral_2, test_data, type = 'response')
test_data$Predicted_Sentiment <- ifelse(predict_neutral > 0.53, 0, 2) # neutral (0) or non-neutral (2: Temporary Value).

# Stap 2: Binary Classification for Positive vs. Negative (Non-Neutral Tweets Only)
non_neutral_tweets <- test_data[test_data$Predicted_Sentiment == 2, ]

# Use the modelPositive_2 model to predict probabilities of being positive for the non-neutral tweets.
predict_positive <- predict(modelPositive_2, non_neutral_tweets, type = 'response')
non_neutral_tweets$Predicted_Sentiment <- ifelse(predict_positive > 0.5, 1, -1) # positive (1) or negative (-1)

# Step 3: Combine Results
# Replace the Predicted_Sentiment values for non-neutral tweets in the original dataframe
test_data[test_data$Predicted_Sentiment == 2, ] <- non_neutral_tweets
```

```{r}
tab_lg <- table(test_data$Predicted_Sentiment, test_data$sentiment)

print(tab_lg )
accuracy_lg = (tab_lg[1,1]+tab_lg[2,2] + tab_lg[3,3])/sum(tab_lg)

accuracy_lg
```

```{r}
# Additional 
# Combine all 3 models into one model and try finding accuracy 

# Touch later 
model <- multinom(sentiment ~ ., data = data)

```

## Commentary


# Second Model: Random Forest 
This model takes a while to train.
```{r}

# subset columns in train so that it only has the prediction column, sentiment, and the rest of the words
train_rf <- train_data

train_rf$sentiment <- as.factor(train_rf$sentiment)
# print(class(train_rf$sentiment))
rf_model <- randomForest( sentiment~., data=train_rf , ntree = 50 , ncores = 8 )
# changing ntree to 100 does not make much of a difference

# summary(modelPositive)

# importance(modelPositive)
varImpPlot(rf_model) # plot

# Test Model on original Training Data
predict_rf_train <- predict(rf_model, newdata=train_data , type = "response" )

tab_rf_train <- table(predict_rf_train, train_data$sentiment)

print(tab_rf_train )
Accuracy = (tab_rf_train[1,1]+tab_rf_train[2,2] + tab_rf_train[3,3])/sum(tab_rf_train)
Accuracy # [1] 0.8387231


# Test Model on Test Data
test_data$sentiment <- as.factor(test_data$sentiment)
predict_rf <- predict(rf_model, newdata=test_data , type = "response" )

tab_rf <- table(predict_rf, test_data$sentiment)

print(tab_rf )
Accuracy = (tab_rf[1,1]+tab_rf[2,2] + tab_rf[3,3])/sum(tab_rf)
Accuracy # [1] 0.651129
```


# Model 3 Neural Network
```{r}

# subset columns in train so that it only has the prediction column, sentiment, and the rest of the words
train_nn <- train_data
X_train = subset(train_nn , select = -c(sentiment) )

# check data type of features is integer
# column_types <- all(sapply(X_train, is.integer))
# print(column_types )
Y_train = to_categorical( subset(train_nn , select = c(sentiment) ) , num_classes = 3 )
# print(Y_train)

print( ncol(X_train))
print( ncol(Y_train))




# print( length( unique( colnames(X_train) ) )==  length( colnames(X_train) ) )
# print( class(X_train[,1]) )

test_nn <- test_data
X_test = subset(test_nn , select = -c(sentiment , Predicted_Sentiment ) )
# test_nn$sentiment
Y_test = to_categorical( subset(test_nn , select = c(sentiment) ) , num_classes = 3)
print( ncol(X_test))
print( ncol(Y_test))

max_category_value <- 1
input_dim <- ncol(X_train)
embedding_dim <- 128

nn_model = keras_model_sequential()

nn_model %>%
  layer_dense(units = 64, activation = 'relu' , input_shape = input_dim ,
              bias_regularizer=regularizer_l1(l=0.1),
              activity_regularizer=regularizer_l1(l=.05) ) %>%
  
  layer_dense(units = 64, activation = 'relu',
              bias_regularizer=regularizer_l1(l=0.1),
              activity_regularizer=regularizer_l1(l=.05)) %>%
  layer_dense(units = 3 , activation = 'softmax')

nn_model %>% compile(
  loss      = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics   = c('accuracy')
)
nn_model %>% plot_model()
summary(nn_model)



history <- nn_model %>% fit(
  x = as.matrix( X_train ) ,
  y = Y_train ,
  epochs = 100,  # Number of training iterations
  batch_size = 128,  # Batch size for gradient updates
  validation_split = 0.2  # Split data into training and validation sets
)

performance_nn = nn_model %>% evaluate(as.matrix( X_test), Y_test)
print(performance_nn)
accuracy <- performance_nn[2]
print(str_c("Accuracy of the Neural Network model was ",round(accuracy*100,0),"% on the test data"))


```
```{r}
# Load the required libraries
library(keras)

# Example data with categorical values (0 or 1) and column names
# Replace this with your actual data
input_data <- as.data.frame(matrix(sample(0:3, ncol(X_train) * 14410, replace = TRUE), ncol = ncol(X_train)))
colnames(input_data) <- colnames(X_train)

print( length(colnames(X_train)) )
print(ncol(X_train) )

# Example labels (categorical: -1, 0, or 1)
# Replace this with your actual labels
labels <- sample(-1:1, 14410, replace = TRUE)


# print(input_data)
# print(labels)
# Define the input dimension (number of features)
input_dim <- ncol(input_data)

# Create a sequential neural network model
nn_model <- keras_model_sequential()

# Add a dense layer (adjust the number of units and activation as needed)
nn_model %>%
  layer_dense(units = 128, activation = 'relu', input_shape = c(input_dim))

# Add more hidden layers as needed
nn_model %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dense(units = 32, activation = 'relu')

# Output layer with 3 units for -1, 0, and 1 categories
nn_model %>%
  layer_dense(units = 3, activation = 'softmax')

# Compile the model
nn_model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',  # Use categorical cross-entropy for multiclass classification
  metrics = c('accuracy')
)

# Convert labels to one-hot encoding (assuming labels are -1, 0, or 1)
labels <- to_categorical(labels + 1, num_classes = 3)  # +1 to map -1, 0, 1 to 0, 1, 2

# Train the model (replace 'input_data' and 'labels' with your actual data and labels)
history <- nn_model %>% fit(
  x = as.matrix(input_data),
  y = labels,
  epochs = 100,  # Number of training iterations
  batch_size = 32,  # Batch size for gradient updates
  validation_split = 0.2  # Split data into training and validation sets
)


```


## Predictions for Neural Network Model
```{r}
performance_nn = nn_model %>% evaluate(as.matrix( X_test), Y_test)
print(performance_nn)
print(str_c("Accuracy of the Neural Network model was ",round(accuracy*100,0),"% on the test data"))
```


# Conclusion