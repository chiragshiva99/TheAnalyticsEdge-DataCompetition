---
title: "The Analytics Edge Data Competition, Twitter Sentiment Analysis"
author: "Chirag Shivakumar and Tan Zen Sheen"
date: "2023-09-02"
output: html_document
---

In this project, we are given a labelled dataset of Tweets from twitter.
In the *train.csv* data, tweets have been labelled with a Sentiment score of 0, 1, or 2, representing sentiments of negative, neutral or positive respectively.

Through applying various Machine Learning Techniques, my team and I are tasked to create a data model predict the sentiments of the tweets.

```{r setup, include=FALSE} 
knitr::opts_chunk$set(echo = TRUE,warning=FALSE, message=FALSE)
```

# Load Packages needed for this Project, install if needed.
```{r error=TRUE, message=FALSE}
library(stringr) 
library(spellcheckr)
library(dplyr)
library(tm)
library(SnowballC)
library(ROCR)
library(caTools)
library(glmnet)
library(ROCR)
library(nnet)
library(randomForest)
library(keras)
library(deepviz)
```

# Data Cleaning and Processing
In cleaning and processing the data of tweets, we decided to go one step further, extracting information of emoticons from the tweets, removing website links, and detect some incorrectly spelled words.
The entire script for Data Cleaning and Processing is saved in ```clean.R```.

```{r}
source("clean.R")
```

# How clean.R it works

## Extracting of punctuations and emoticons from Tweets and assigning value to new columns that represent certain emotions.
Besides organizing the streets of tweets into a dataframe that stores the frequency of words in each tweet, ```clean.R``` helps to extract the emoticons in the tweets by detecting them and assigning a value to its associated emotion.

To do this, we add new columns of sadd, smle, qsnm, excl and dott to the output dataframe. The new columns are computed to have the following meanings: 

* *sadd*: Means 'sad' and refers to the emotion of sadness. This column will have a value of 1 when there is an emoji that represents the emotion of sadness, anger or disappointment. 

* *smle*: Means 'smile' and refers to the emotion of happiness. This column will have a value of 1 when there is an emoji that represents the emotion of happiness or excitement. 

* *excl*: Refers to the exclamation mark "!". This column will have a value of 1 when there is a "!" in the tweet. 

* *qsm* : Refers to question mark "?". This column will have a value of 1 when there is a "?" in the tweet.

* *dott*: Refers to multiple continuous dots in tweets. This column will have a value of 1 when there are consecutive periods in a tweet(I.e. "...")

Using the emoticons data set from https://github.com/matbun/Emoticons-dataset , tweets from the data are being screened through to detect emoticons. The emoticons dataset has actually been labelled with values of 1 or -1, representing positive and negative sentiments respectively.

Using these labels, we associate the positive emotions (that are labelled '1') to have a smle value of 1 and a negative emoticons (that are labelled '-1') to have the sadd value of 1. If there are no emoticons present in the tweet, the 'smle' and 'sadd' columns would be labelled 0.

```{r}
emo_data <- read.csv("emoticons_long.csv")
print( head(emo_data) )
```

## Identifying punctuation through clean_punc function from clean.R
In clean.R, the ```clean_punc``` function takes in a string and outputs a list which contains *"words'* and *'punc'*.

The *'$punc'* output variable stores the computed columns of *'sadd'*, *'smle'*, *'qsnm'*, *'excl'*, and *'dott'*.

```{r}
# test code for clean_punc function

str1 <- clean_punc('I love Machine Learning!:)')
print( str1$punc)

str2 <- clean_punc('Why is my model takign so long to train...? :(')
print( str2$punc)

```

## Correction of words with 3 of the same consecutive characters and removal of website url in Tweets.
As the tweets contain numerous typos, it is wise to do some cleaning of the tweets to ensure that the words extracted are correctly spelled as much as possible.

As running a spelling checker through all the words in the tweet would be computationally very intensive, we decided to simplify this by simply checking for words that have 3 of the same consecutive letters, then correcting their spelling.

Website URLs are also removed from the tweets as they are not important for the prediction of the model.

```{r}

str3 <- clean_punc( " yeeeeah .. . and it was in into too  seen Empire top 100 computer games? http://www.empireonline.com/100greatestgames/" )

print( str3$words)

str4 <- clean_punc("i kno look ? :) (: i doooo!!!!!!!!!! yall partyin with out me htttp://www.google.com" )
print( str4$words)

```
The ```clean.r``` script  also also changes the labels of data such that the Sentiment values are now -1, 0, and 1 to represent negative, neutral and positive tweets, from its previous values of 0, 1, and 2.


# Execute Cleaning and Data Processing on test and train datasets.
*Note of Caution: Running the code below will take about 5 minutes.*

Running the code below will execute clean.R on the ```train.csv```, screening through every tweet with the ```clean_punc``` function as demonstrated above, and preparing the data as a dataframe for the training and testing of the model. This will overwrite and save the cleaned data as ```data.csv```. 

In the original competition, an unlabelled test data set, test.csv is given for us to make our predictions on, and submit out predictions onto Kaggle. As the labels from the test data are not available, we shalll ignore the test data and instead split the labelled data for training and testing. 
```{r}
# train <- prep_data( "train.csv" )
# train_data <- train$data
# train_data$sentiment <- train$sentiment
# write.csv( train_data,"data.csv", row.names = FALSE)
```

# Load Processed Data and split data for Training and Testing
```{r}
data <- read.csv("train_data.csv")
set.seed(123)
split_indices  <- sample.split(data , SplitRatio=0.7)
train_data <- data[split_indices, ]
test_data <- data[!split_indices, ]

```

# Understanding the data

Let us look at the first few rows and last few columns of the testing and training dataset.
```{r}
print(head(train_data)[ , (ncol(train_data) - 9):ncol(train_data) ])

```

```{r}

print( "Print first few rows and last 10 columns of test data")
print( head(test_data)[ , (ncol(test_data) - 9):ncol(test_data) ])

```

Before proceeding with the Machine Learning models, some things to understand and check about the data are:

* What are the dimensions of the dataset?

* What are the possible frequency values of words that can show up in each word column?

* Are there any NULL values in the dataset?

```{r}

# Getting dimensions of both datasets
print( paste0("The test data set has " , nrow(test_data) , " rows and " , ncol(test_data) , " columnes."  ) )

print( paste0("The train data set has " , nrow(train_data) , " rows and " , ncol(train_data) , " columnes." ))


# Getting unique values of original data 
# Function to get unique values of a column
get_unique_values <- function(column) {
  unique_values <- unique(column)
  return(unique_values)
}

# Initialize an empty list to store unique values for each column
unique_values_list <- list()

# Flatten the dataframe into a vector
data_vector <- unlist(subset(data , select = -c(sentiment) ))

# Get unique values from the vector
unique_values <- unique(data_vector)


print( paste0("The unsplitted data has " , length( unique_values ) , " unique values." ) )
print( paste0("The unique values are " , paste(unique_values , collapse = " ") , ".") )


# Check for NULL values
has_null_values <- anyNA(data)
# Print the result (TRUE if any missing values, FALSE otherwise)
print(paste0("Check for NULL values: " , has_null_values,"." ) )
```

We are now confident that the data is ready to be used for model training and testing. 


# Model 1: Double Logistic Regression
In the first model, we will be using Logistic Regression for predicting neutral and non-neutral (positive or negative) tweets, before predicting positive and negative tweets.

The motivation behind this model is that based on the principle that tweets that are less likely to negative are more likely to be positive, and vice versa. Thus, through predicting if data is neutral or non-neutral first, we can filter out the predicted neutral rows from the data, and predict if the rest of the data is positive or negative.


## Data Preparation for 1st Model
Before training this model, we need to prepare 2 datasets for 2 models:

* Neutral train and test datasets : Contains the entire data, labelled by whether they are neutral or non-neutral. 

* Positive train and test datasets: Contains a subset of the entire data that is non-neutral, labelled by whether they are positive or negative. 

```{r}
# make copies of train and data so we do not modify the orignial data
train_lr <- train_data
test_lr <- test_data

# Assign Positive, Neutral and Negative columns to training data, based on Sentiment Scores
train_lr$Neutral <-as.factor(train_data$sentiment==0)
# Do the same for test data, for testing of model
test_lr$Neutral <-as.factor(test_data$sentiment==0)


# Data for Neutral Model
train_Neutral <- subset( train_lr , select = -c(sentiment) )
test_Neutral <- subset( test_lr , select = -c(sentiment) )



# Data for Non-Neutral Model
# remove neutral tweet, so we are now left with positive and negative tweets
train_Positive <- train_lr[ train_lr$Neutral == FALSE , ]
test_Positive <- test_lr[ test_lr$Neutral == FALSE , ]

train_Positive$Positive <- as.factor( train_Positive$sentiment == 1 )
test_Positive$Positive <- as.factor( test_Positive$sentiment == 1 )

train_Positive <- subset( train_Positive , select = -c(Neutral , sentiment ) )
test_Positive <- subset( test_Positive , select = -c(Neutral , sentiment ) )

# Printing last 10 columns and first 6 rows of train_Neutral and train_positive
print( head(train_Neutral)[ , (ncol(train_Neutral) - 9):ncol(train_Neutral) ])
print( head(train_Positive[ , (ncol(train_Positive) - 9):ncol(train_Positive) ]))

```

From the above, the data for the neutral model is labelled in the "Neutral" column while the data for the non-neutral model is labelled in the "Positive" column.

In the training of both Logistic Regression Models, we start by training the model on all features, then filtering out the features that have p-values of 0.05 and below to be used for the final model.

## Training of Neutral Model, seperating neutral from non-neutral Tweets.
```{r}
# Neutral Model
modelNeutral <- glm(Neutral~0+., data=train_Neutral, family=binomial)
# summary(modelNeutral) # AIC: 17588


neutral_pVal <- summary(modelNeutral)$coefficients[,4] #p values
neutral_sigVal <- names(neutral_pVal[neutral_pVal <= 0.05]) #5% level

modelNeutral_2 <- glm(as.formula(
  paste0("Neutral ~ ",
         paste0(neutral_sigVal, collapse = " + "),
         "-1")  
) , data = train_Neutral, family = binomial)

print( modelNeutral_2 )

# Testing of results against the original train_Neutral data
prob_neutal_train <- predict( modelNeutral_2 , type = 'response' )
predictions_neutral_train <- prediction(prob_neutal_train, train_Neutral$Neutral)

# Plot ROC curve to visualize
neutral_roc_curve <- performance(predictions_neutral_train, "tpr", "fpr")
plot(neutral_roc_curve, main = "ROC Curve", print.auc = TRUE,colorize=T,print.cutoffs.at=c(0,0.3,0.4,0.5,0.55,0.6,1),text.adj=c(-.02,1.7))


# Based on observation of the ROC curve, we use the cutoff probability value of 0.47 with the highest TPR and lowest FPR.

# As the model needs to higher TRUE Negative for more predicted non-neutral rows to be seperate in the second model, we choose a cut-off value of 0.5 that has a better balance of a lower False Positive Rate

# Make Predictions on train data based on cutoff value of 0.5
tab_neutral_train <- table(prob_neutal_train>0.5,train_Neutral$Neutral)
neutral_train_Accuracy = (tab_neutral_train[1,1]+tab_neutral_train[2,2])/sum(tab_neutral_train)
print( paste0( "The accuracy of the neutral model on the train data is ",round( neutral_train_Accuracy*100 ,2 ) , "%.") )


# Testing of results in the test_Neutral data
# Neutral Model
prob_neutal_test <- predict( modelNeutral_2 , test_Neutral , type = 'response' )
tab_neutral_test <- table(prob_neutal_test>0.5,test_Neutral$Neutral) 
neutral_test_Accuracy = (tab_neutral_test[1,1]+tab_neutral_test[2,2])/sum(tab_neutral_test)
print( paste0( "The accuracy of the neutral model on the test data is " ,round( neutral_test_Accuracy*100 ,2 ) , "%.") )

```

## Training of Positive Model, seperating Positive from Negative Tweets.
```{r}

modelPositive <- glm(Positive~0+., data=train_Positive, family=binomial)

positive_pVal <- summary(modelPositive)$coefficients[,4] #p values
positive_sigVal <- names(positive_pVal[positive_pVal <= 0.05]) #5% level

modelPositive_2 <- glm(as.formula(
  paste0("Positive ~ ",
         paste0(positive_sigVal, collapse = " + "),
         "-1")  
) , data = train_Positive, family = binomial)

print( modelPositive_2 )

# Testing of results against the original train_Positive data
prob_positive_train <- predict( modelPositive_2 , type = 'response' )
predictions_positive_train <- prediction(prob_positive_train, train_Positive$Positive)

# Plot ROC curve to visualize
positive_roc_curve <- performance(predictions_positive_train, "tpr", "fpr")
plot(positive_roc_curve, main = "ROC Curve", print.auc = TRUE,colorize=T,print.cutoffs.at=c(0,0.3,0.4,0.5,0.55,0.6,0.7,0.8,1),text.adj=c(-.02,1.7))

# Based on observation of the ROC curve, we use the cutoff probability value of 0.6 with the highest TPR and lowest FPR.


# Make Predictions on train data based on cutoff value of 0.51
tab_Positive_train <- table(prob_positive_train>0.51,train_Positive$Positive)
positive_train_Accuracy = (tab_Positive_train[1,1]+tab_Positive_train[2,2])/sum(tab_Positive_train)
positive_train_Accuracy # [1] 0.8312757


print( paste0( "The accuracy of the positive model on the train data is " ,round( positive_train_Accuracy*100 ,2 ) , "%.") )

# Testing of results in the test_Positive data
# Positive Model
prob_positive_test <- predict( modelPositive_2 ,test_Positive, type = 'response' )
tab_positive_test <- table(prob_positive_test>0.51,test_Positive$Positive) 
positive_test_Accuracy = (tab_positive_test[1,1]+tab_positive_test[2,2])/sum(tab_positive_test)
print( paste0( "The accuracy of the neutral model on the test data is " ,round( positive_test_Accuracy*100 ,2 ) , "%.") )

```


## Model Predictions for Model1
In the previous code, we test on the model on the test data after it has been spitted by neutral and non-neutral labels. In reality, we would not not know which rows in the test data are neutral or non-neutral as we have to directly predict the sentiments of positive, negative, and neutral at the same time. 

To do this, we first use the first Neutral Model to predict the neutral data, and filter out the non-neutral data. Then, we use the second Positive Model to predict positive and negative data.


```{r}

# Predict Neutral vs. Non-Neutral Labels
lr_prob_neutral <- predict(modelNeutral_2, train_lr, type = 'response')
tab_neutral <- table( lr_prob_neutral > 0.5, as.factor( train_lr$sentiment == 0))

# Create a new Column, "Neutral", for separating of neutral and non-neutral predictions
train_lr$Neutral <- ifelse(lr_prob_neutral > 0.5 , TRUE , FALSE )
train_lr$Predicted_Sentiment <- ifelse( train_lr$Neutral  == TRUE , 0, 2) # neutral (0) or non-neutral (2: Temporary Value).

# Save predicted non-neutral tweets in a separate dataframe for predicting of positive and negative data in the second model
non_neutral_tweets <- train_lr[train_lr$Predicted_Sentiment == 2, ]

# Predict Non-Neutral (Positive vs. Negative) Labels
lr_predict_positive <- predict(modelPositive_2, non_neutral_tweets , type = 'response')

non_neutral_tweets$Predicted_Sentiment <- ifelse( lr_predict_positive>0.51 , 1, -1)


# Combining all labels together
train_lr[train_lr$Predicted_Sentiment == 2, ] <- non_neutral_tweets

tab_lg <- table(train_lr$Predicted_Sentiment, train_lr$sentiment)
print(tab_lg )

accuracy_lg_test = (tab_lg[1,1]+tab_lg[2,2] + tab_lg[3,3])/sum(tab_lg)
print(paste0("The accuracy of the Dual Logistic Regression is ", round( accuracy_lg_test*100,2 ) , "% on the train data" ) )

```
When tested on the train data, the Dual Logistic Regression model only got an accuracy of 55.83%. With this, we would not be expecting an accuracy that is much lower when the model is used on the test data.


```{r}

# Predict Neutral vs. Non-Neutral Labels
lr_prob_neutral <- predict(modelNeutral_2, test_lr, type = 'response')
tab_neutral <- table( lr_prob_neutral > 0.5, as.factor( test_lr$sentiment == 0))
print( tab_neutral )

# Create a new Column, "Neutral", for separating of neutral and non-neutral predictions
test_lr$Neutral <- ifelse(lr_prob_neutral > 0.5 , TRUE , FALSE )
test_lr$Predicted_Sentiment <- ifelse( test_lr$Neutral  == TRUE , 0, 2) # neutral (0) or non-neutral (2: Temporary Value).

# Save predicted non-neutral tweets in a separate dataframe for predicting of positive and negative data in the second model
non_neutral_tweets <- test_lr[test_lr$Predicted_Sentiment == 2, ]

# Predict Non-Neutral (Positive vs. Negative) Labels
lr_predict_positive <- predict(modelPositive_2, non_neutral_tweets , type = 'response')

non_neutral_tweets$Predicted_Sentiment <- ifelse( lr_predict_positive>0.51 , 1, -1)


# Combining all labels together
test_lr[test_lr$Predicted_Sentiment == 2, ] <- non_neutral_tweets

tab_lg <- table(test_lr$Predicted_Sentiment, test_lr$sentiment)
print(tab_lg )

accuracy_lg_test = (tab_lg[1,1]+tab_lg[2,2] + tab_lg[3,3])/sum(tab_lg)
print(paste0("The accuracy of the Dual Logistic Regression is ", round( accuracy_lg_test*100,2 ) , "% on the test data" ) )

```
Despite the low accuracy when tested on the train data, the accuracy results of the model was fairly consistent when tested on the test data.

# Model 2: Random Forest 

Here, we use a Random Forest Model to predict 3 categories directly. We would be expecting a higher accuracy in using this model due to clever use of decision trees and bootstrapping to make good predictions without overfitting. 
```{r}

# subset columns in train so that it only has the prediction column, sentiment, and the rest of the words
train_rf <- train_data

train_rf$sentiment <- as.factor(train_rf$sentiment)
# print(class(train_rf$sentiment))
rf_model <- randomForest( sentiment~., data=train_rf , ntree = 50 , ncores = 8 )
# changing ntree to 100 does not make much of a difference

# summary(modelPositive)

# importance(modelPositive)
varImpPlot(rf_model) # plot

# Test Model on original Training Data
predict_rf_train <- predict(rf_model, newdata=train_data , type = "response" )

tab_rf_train <- table(predict_rf_train, train_data$sentiment)

print(tab_rf_train )
accuracy_rf_train = (tab_rf_train[1,1]+tab_rf_train[2,2] + tab_rf_train[3,3])/sum(tab_rf_train)
print( paste0("The accuracy of the Random Forest Model is ", round(accuracy_rf_train*100,2) , "% on the train data." ))


# Test Model on Test Data
test_data$sentiment <- as.factor(test_data$sentiment)
predict_rf <- predict(rf_model, newdata=test_data , type = "response" )

tab_rf <- table(predict_rf, test_data$sentiment)

print(tab_rf )
accuracy_rf_test = (tab_rf[1,1]+tab_rf[2,2] + tab_rf[3,3])/sum(tab_rf)

print( paste0("The accuracy of the Random Forest Model is ", round(accuracy_rf_test*100,2) , "% on the test data." ))
```


# Model 3 Neural Network
Here, we explore using a Neural Network to see if it would be better than the previous two models. As there may be numerous permutations of the number of layers and the activation functions of each layer, we will be comparing the effectiveness of 2 different Neural Network Models. They are as such:

* *Model 3.1*: 1 Hidden Layer with ReLu Activation function, and a final layer of Softmax Activation.

* *Model 3.2*: 1 Hidden Layer with Softmax Activation function, and a final layer of Softmax Activation.

Both models have the same input and output parameters and L1 regularizer for the bias and activity variables of the hidden layer.

## Model 3.1: Neural Network Model with ReLu Hidden Layer
```{r}

# subset columns in train so that it only has the prediction column, sentiment, and the rest of the words
train_nn <- train_data
X_train = subset(train_nn , select = -c(sentiment) )

# check data type of features is integer
# column_types <- all(sapply(X_train, is.integer))
# print(column_types )
Y_train = to_categorical( subset(train_nn , select = c(sentiment) ) , num_classes = 3 )
# print(Y_train)

print( ncol(X_train))
print( ncol(Y_train))

# print( length( unique( colnames(X_train) ) )==  length( colnames(X_train) ) )
# print( class(X_train[,1]) )

test_nn <- test_data
X_test = subset(test_nn , select = -c(sentiment ) )
# test_nn$sentiment
Y_test = to_categorical( subset(test_nn , select = c(sentiment) ) , num_classes = 3)
print( ncol(X_test))
print( ncol(Y_test))

max_category_value <- 1
input_dim <- ncol(X_train)
embedding_dim <- 128

nn_model = keras_model_sequential()

nn_model %>%
  layer_dense(units = 64, activation = 'relu' , input_shape = input_dim ,
              bias_regularizer=regularizer_l1(l=0.1),
              activity_regularizer=regularizer_l1(l=.05) ) %>%
  layer_dense(units = 3 , activation = 'softmax')

nn_model %>% compile(
  loss      = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics   = c('accuracy')
)
nn_model %>% plot_model()
summary(nn_model)



history <- nn_model %>% fit(
  x = as.matrix( X_train ) ,
  y = Y_train ,
  epochs = 100,  # Number of training iterations
  batch_size = 128,  # Batch size for gradient updates
  validation_split = 0.2  # Split data into training and validation sets
)


performance_nn_train = nn_model %>% evaluate(as.matrix( X_train), Y_train)
print(performance_nn_train)
accuracy_nn_train <- performance_nn_train[2]
print(str_c("Accuracy of the Neural Network model was ",round(accuracy_nn_train*100,2),"% on the train data"))

performance_nn_train = nn_model %>% evaluate(as.matrix( X_test), Y_test)
print(performance_nn_train)
accuracy_nn_train <- performance_nn_train[2]
print(str_c("Accuracy of the Neural Network model was ",round(accuracy_nn_train*100,2),"% on the test data"))
```

## Model 3.2: Neural Network Model with Softmax Hidden Layer
```{r}

# subset columns in train so that it only has the prediction column, sentiment, and the rest of the words
train_nn <- train_data
X_train = subset(train_nn , select = -c(sentiment) )

# check data type of features is integer
# column_types <- all(sapply(X_train, is.integer))
# print(column_types )
Y_train = to_categorical( subset(train_nn , select = c(sentiment) ) , num_classes = 3 )
# print(Y_train)

print( ncol(X_train))
print( ncol(Y_train))

# print( length( unique( colnames(X_train) ) )==  length( colnames(X_train) ) )
# print( class(X_train[,1]) )

test_nn <- test_data
X_test = subset(test_nn , select = -c(sentiment ) )
# test_nn$sentiment
Y_test = to_categorical( subset(test_nn , select = c(sentiment) ) , num_classes = 3)
print( ncol(X_test))
print( ncol(Y_test))

max_category_value <- 1
input_dim <- ncol(X_train)
embedding_dim <- 128

nn_model = keras_model_sequential()

nn_model %>%
  layer_dense(units = 64, activation = 'softmax', input_shape = input_dim ,
              bias_regularizer=regularizer_l1(l=0.1),
              activity_regularizer=regularizer_l1(l=.05)) %>%
  layer_dense(units = 3 , activation = 'softmax' )

nn_model %>% compile(
  loss      = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics   = c('accuracy')
)
nn_model %>% plot_model()
summary(nn_model)



history <- nn_model %>% fit(
  x = as.matrix( X_train ) ,
  y = Y_train ,
  epochs = 100,  # Number of training iterations
  batch_size = 128,  # Batch size for gradient updates
  validation_split = 0.2  # Split data into training and validation sets
)


performance_nn_train = nn_model %>% evaluate(as.matrix( X_train), Y_train)
print(performance_nn_train)
accuracy_nn_train <- performance_nn_train[2]
print(str_c("Accuracy of the Neural Network model was ",round(accuracy_nn_train*100,2),"% on the train data"))

performance_nn_train = nn_model %>% evaluate(as.matrix( X_test), Y_test)
print(performance_nn_train)
accuracy_nn_train <- performance_nn_train[2]
print(str_c("Accuracy of the Neural Network model was ",round(accuracy_nn_train*100,2),"% on the test data"))

```
The Neural Network models 3.1 and 3.2 have accuracy scores of 61.5% and 63.89%. It turns out that the model that uses Softmax activation for the hidden layer performed slightly better.

It is surprising that the accuracy of the Neural Network did not perform as well as the Random Forest model. 

# Conclusion

In this project, we coded and used the ```clean.R``` script to clean and process the ```train.csv``` data, which outputs train_data.csv for model training and testing. Through data wrangling, we extracted information of the punctuation and emoticons from the tweets into categorical columns that represent certain emotions which would be useful for the models in predicting the sentiment of the tweets.

In the model training and testing, we experimented with a Double Logistic Regression Model, Random Forest Model and Neural Network Model. Of the 3 models used, we found that the Random Forest Model worked the best with the highest accuracy on the test dataset.